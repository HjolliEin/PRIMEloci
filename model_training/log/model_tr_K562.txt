[zmk214@scarbrna01fl model_training]$ python3 train_model.py --model_name model_tr_K562 --model_dir ./ --profile_main_dir ../../model_PRIMEloci/1_prep_data_for_training/K562_profiles --keep_word tr
profiles_subtnorm_pos_tr_K562_C1.csv contains 21572 rows
profiles_subtnorm_pos_tr_K562_C2.csv contains 22474 rows
profiles_subtnorm_pos_tr_K562_C3.csv contains 22533 rows
profiles_subtnorm_pos_tr_K562_N4.csv contains 54684 rows
profiles_subtnorm_pos_tr_K562_N5.csv contains 49292 rows
profiles_subtnorm_pos_tr_K562_N6.csv contains 59265 rows
profiles_subtnorm_neg_tr_K562_C1.csv contains 21572 rows
profiles_subtnorm_neg_tr_K562_C2.csv contains 22474 rows
profiles_subtnorm_neg_tr_K562_C3.csv contains 22533 rows
profiles_subtnorm_neg_tr_K562_N4.csv contains 54684 rows
profiles_subtnorm_neg_tr_K562_N5.csv contains 49292 rows
profiles_subtnorm_neg_tr_K562_N6.csv contains 59265 rows
Training data shape:
(459640, 401)
(459640,)
[LightGBM] [Info] Number of positive: 229820, number of negative: 229820
[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.099702 seconds.
You can set `force_row_wise=true` to remove the overhead.
And if memory is not enough, you can set `force_col_wise=true`.
[LightGBM] [Info] Total Bins 102254
[LightGBM] [Info] Number of data points in the train set: 459640, number of used features: 401
[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf
[LightGBM] [Warning] No further splits with positive gain, best gain: -inf

Training set score: 0.8116